{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGBJegvBuXV"
   },
   "source": [
    "# Assignment: Video Quality Inference\n",
    "\n",
    "To this point in the class, you have learned various techniques for leading and analyzing packet captures of various types, generating features from those packet captures, and training and evaluating models using those features.\n",
    "\n",
    "In this assignment, you will put all of this together, using a network traffic trace to train a model to automatically infer video quality of experience from a labeled traffic trace.\n",
    "\n",
    "## Part 1: Warmup\n",
    "\n",
    "The first part of this assignment builds directly on the hands-on activities but extends them slightly.\n",
    "\n",
    "### Extract Features from the Network Traffic\n",
    "\n",
    "Load the `netflix.pcap` file, which is a packet trace that includes network traffic. \n",
    "\n",
    "Click [here](https://github.com/noise-lab/ml-systems/blob/main/docs/notebooks/data/netflix.pcap) to download `netflix.pcap`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 141471 packets from netflix.pcap\n"
     ]
    }
   ],
   "source": [
    "import scapy.all as scapy\n",
    "# from netml.pparser.process_pcap import from_pcap\n",
    "from netml.pparser.parser import PCAP\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pcap_file = 'netflix.pcap'\n",
    "try:\n",
    "    packets = scapy.rdpcap(pcap_file)\n",
    "    print(f\"Successfully loaded {len(packets)} packets from {pcap_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {pcap_file} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the pcap: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique domains: 39\n",
      "543458527._teamviewer._tcp.local\n",
      "Diegos-MacBook-Air.local\n",
      "_privet._tcp.local\n",
      "accounts.google.com\n",
      "adservice.google.fr\n",
      "apis.google.com\n",
      "assets.nflxext.com\n",
      "codex.nflxext.com\n",
      "customerevents.netflix.com\n",
      "db._dns-sd._udp.0.43.168.192.in-addr.arpa\n",
      "dr._dns-sd._udp.0.43.168.192.in-addr.arpa\n",
      "fonts.gstatic.com\n",
      "freegeoip.net\n",
      "google.com\n",
      "googleads.g.doubleclick.net\n",
      "ipv4-c001-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c005-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c024-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c063-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c069-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c071-cdg001-ix.1.oca.nflxvideo.net\n",
      "ipv4-c072-cdg001-ix.1.oca.nflxvideo.net\n",
      "mpittoni-macbook._ftp._tcp.local\n",
      "mpittoni-macbook._sftp-ssh._tcp.local\n",
      "occ-0-56-55.1.nflxso.net\n",
      "push.prod.netflix.com\n",
      "r._dns-sd._udp.0.43.168.192.in-addr.arpa\n",
      "r4---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "ssl.gstatic.com\n",
      "tp-s.nflximg.net\n",
      "update.googleapis.com\n",
      "www.google.com\n",
      "www.google.fr\n",
      "www.googleadservices.com\n",
      "www.gstatic.com\n",
      "www.netflix.com\n",
      "www.youtube.com\n",
      "yt3.ggpht.com\n",
      "ytimg.l.google.com\n"
     ]
    }
   ],
   "source": [
    "# Seeing all domain names\n",
    "import scapy.all as scapy\n",
    "\n",
    "pcap_file = \"netflix.pcap\"\n",
    "packets = scapy.rdpcap(pcap_file)\n",
    "\n",
    "domains = set()\n",
    "for pkt in packets:\n",
    "    if pkt.haslayer(scapy.DNS) and pkt[scapy.DNS].qr == 0:  # DNS query\n",
    "        try:\n",
    "            domains.add(pkt[scapy.DNS].qd.qname.decode('utf-8').strip('.'))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"Total unique domains: {len(domains)}\")\n",
    "for d in sorted(domains):\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOPPsKpYB6PS"
   },
   "source": [
    "### Identifying the Service Type\n",
    "\n",
    "Use the DNS traffic to filter the packet trace for Netflix traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 15 Netflix-related IP addresses.\n",
      "{'52.210.19.176', '34.252.77.54', '198.38.120.167', '52.208.128.101', '198.38.120.130', '198.38.120.166', '52.48.148.78', '198.38.120.164', '198.38.120.137', '198.38.120.134', '52.210.133.255', '198.38.120.153', '52.19.39.146', '198.38.120.162', '52.48.8.150'}\n",
      "\n",
      "Filtered down to 138633 Netflix-related packets.\n",
      "Saved filtered Netflix traffic to netflix_only.pcap\n"
     ]
    }
   ],
   "source": [
    "# Based on the domains observed, we can filter for Netflix-related traffic\n",
    "netflix_domains = [\n",
    "    \"nflxvideo.net\",\n",
    "    \"nflxext.com\",\n",
    "    \"nflxso.net\",\n",
    "    \"nflximg.net\",\n",
    "    \"netflix.com\"\n",
    "]\n",
    "\n",
    "netflix_ips = set()\n",
    "\n",
    "# We only want to keep responses: DNS response (qr=1) with answers (ancount > 0)\n",
    "for pkt in packets:\n",
    "    if pkt.haslayer(scapy.DNS) and pkt[scapy.DNS].qr == 1 and pkt[scapy.DNS].ancount > 0:\n",
    "        for i in range(pkt[scapy.DNS].ancount):\n",
    "            answer = pkt[scapy.DNS].an[i]\n",
    "            \n",
    "            # I had to use Gemini to do this chunk.\n",
    "            # Prompt: \"In Scapy, how do I check if a DNS answer is an 'A' (IPv4) \n",
    "            # record and extract the domain name and IP address?\"\n",
    "            if answer.type == 1: # 'A' (IPv4) record\n",
    "                domain = answer.rrname.decode('utf-8')\n",
    "                if any(nd in domain for nd in netflix_domains):\n",
    "                    netflix_ips.add(answer.rdata)\n",
    "\n",
    "print(f\"\\nIdentified {len(netflix_ips)} Netflix-related IP addresses.\")\n",
    "print(netflix_ips)\n",
    "\n",
    "# Get all packets related to Netflix IPs\n",
    "netflix_packets = []\n",
    "for pkt in packets:\n",
    "    if pkt.haslayer(scapy.IP):\n",
    "        # Keep packets where either source or destination IP is in netflix_ips\n",
    "        if pkt[scapy.IP].src in netflix_ips or pkt[scapy.IP].dst in netflix_ips:\n",
    "            netflix_packets.append(pkt)\n",
    "\n",
    "print(f\"\\nFiltered down to {len(netflix_packets)} Netflix-related packets.\")\n",
    "\n",
    "\n",
    "filtered_pcap_file = 'netflix_only.pcap'\n",
    "scapy.wrpcap(filtered_pcap_file, netflix_packets)\n",
    "print(f\"Saved filtered Netflix traffic to {filtered_pcap_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "### Generate Statistics\n",
    "\n",
    "Generate statistics and features for the Netflix traffic flows. Use the `netml` library or any other technique that you choose to generate a set of features that you think would be good features for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netml.pparser.parser import PCAP\n",
    "from netml.utils.tool import dump_data, load_data\n",
    "\n",
    "pcap = PCAP('netflix_only.pcap', flow_ptks_thres=2)\n",
    "\n",
    "pcap.pcap2flows()\n",
    "\n",
    "# Extract inter-arrival time features\n",
    "pcap.flow2features('IAT', fft=False, header=False)\n",
    "\n",
    "iat_features = pcap.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.714240  0.000158  1.207696  0.017828  0.001739  2.609759  0.001923   \n",
      "1  0.715549  0.000191  1.286184  0.017959  0.002137  2.543728  0.002128   \n",
      "2  0.713235  0.000234  1.128762  0.017737  0.001898  2.618819  0.002096   \n",
      "3  0.768140  0.000545  2.326351  0.015718  0.001761  1.712796  0.981150   \n",
      "4  0.771010  0.000206  2.449168  0.017225  0.002280  1.603454  0.987820   \n",
      "\n",
      "        7             8         9    ...  925  926  927  928  929  930  931  \\\n",
      "0  1.369284  7.958562e+00  0.557696  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1  9.313323  5.603709e-01  0.001295  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2  1.425096  7.973116e+00  0.555151  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3  0.021840  9.536743e-07  0.541786  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4  3.662901  9.536743e-07  0.582064  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   932  933  934  \n",
      "0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 935 columns]\n"
     ]
    }
   ],
   "source": [
    "iat_features_df = pd.DataFrame(iat_features)\n",
    "print(iat_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features\n",
    "pcap.flow2features('STATS', fft=False, header=False)\n",
    "stats_features = pcap.features\n",
    "\n",
    "# Extract sampled-bytes features\n",
    "pcap.flow2features('SAMP_SIZE', fft=False, header=False)\n",
    "samp_size_features = pcap.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1           2           3          4     5     6      7   \\\n",
      "0   14.440177  0.831015   73.683308   88.666667  48.140997  66.0  66.0   69.0   \n",
      "1   14.442865  0.761622   69.099863   90.727273  49.772374  66.0  66.0   72.0   \n",
      "2   14.437420  0.831173   73.697377   88.666667  48.140997  66.0  66.0   69.0   \n",
      "3  138.854511  1.497971  184.560082  123.206731  66.443951  66.0  66.0  200.0   \n",
      "4   74.817607  0.360878   45.203263  125.259259  66.350692  66.0  66.0  200.0   \n",
      "\n",
      "     8      9      10       11  \n",
      "0  66.0  200.0   12.0   1064.0  \n",
      "1  66.0  200.0   11.0    998.0  \n",
      "2  66.0  200.0   12.0   1064.0  \n",
      "3  54.0  200.0  208.0  25627.0  \n",
      "4  54.0  200.0   27.0   3382.0  \n"
     ]
    }
   ],
   "source": [
    "stat_features_df = pd.DataFrame(stats_features)\n",
    "samp_size_features_df = pd.DataFrame(samp_size_features)\n",
    "print(stat_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "**Write a brief justification for the features that you have chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the **STATS** represenation after I used the netml library to extract different types of network features from the Netflix traffic flows. The STATS representation is chosen because it provides useful summary statistics for each flow (packet rate, byte rate, packet size statistics, total packets, and total bytes). These are easy to feed into a model.\n",
    "I also experimented with IAT (inter-arrival times) and SAMP_SIZE (per-interval byte counts) to capture traffic burstiness, which is often correlated with video segment downloads and playback quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBCP_2SBC2xj"
   },
   "source": [
    "### Inferring Segment downloads\n",
    "\n",
    "In addition to the features that you could generate using the `netml` library or similar, add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "\n",
    "Note: If you are using the `netml` library, generating features with `SAMP` style options may be useful, as this option gives you time windows, and you can then simply add the segment download rate to that existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcap.flow2features('SAMP_SIZE', fft=False, header=False)\n",
    "samp_features = pcap.features\n",
    "df_samp = pd.DataFrame(samp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1761.     12901.5   106884.2   286426.45 1093697.6 ]\n"
     ]
    }
   ],
   "source": [
    "# Choosing a threshold\n",
    "import numpy as np\n",
    "values = df_samp.to_numpy().flatten()\n",
    "values = values[values > 0]  \n",
    "print(np.percentile(values, [50, 75, 90, 95, 99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 50_000  # Big bursts likely correspond to video segment downloads\n",
    "segment_counts = (df_samp > threshold).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1761.     12901.5   106884.2   286426.45 1093697.6 ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Video Quality Inference\n",
    "\n",
    "You will now load the complete video dataset from a previous study to train and test models based on these features to automatically infer the quality of a streaming video flow.\n",
    "\n",
    "For this part of the assignment, you will need two pickle files, which we provide for you by running the code below:\n",
    "\n",
    "```\n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the File\n",
    "\n",
    "Load the video dataset pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the File\n",
    "\n",
    "1. The dataset contains video resolutions that are not valid. Remove entries in the dataset that do not contain a valid video resolution. Valid resolutions are 240, 360, 480, 720, 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The file also contains columns that are unnecessary (in fact, unhelpful!) for performing predictions. Identify those columns, and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly explain why you removed those columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data\n",
    "\n",
    "Prepare your data matrix, determine your features and labels, and perform a train-test split on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Your Model\n",
    "\n",
    "1. Select a model of your choice.\n",
    "2. Train the model using your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Your Model\n",
    "\n",
    "Perform hyperparameter tuning to find optimal parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Model\n",
    "\n",
    "Evaluate your model accuracy according to the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1 Score\n",
    "3. Confusion Matrix\n",
    "4. ROC/AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict the Ongoing Resolution of a Real Netflix Session\n",
    "\n",
    "Now that you have your model, it's time to put it in practice!\n",
    "\n",
    "Use a preprocessed Netflix video session to infer **and plot** the resolution at 10-second time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Cd1i3qtplCsVAB9qTjxA",
   "collapsed_sections": [],
   "name": "pcap_processing_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
